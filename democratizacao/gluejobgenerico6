import sys, os, json, uuid, logging, boto3
from datetime import datetime, timedelta
from urllib.parse import urlparse

import pyspark.sql.functions as F
import pyspark.sql.types as T
from pyspark.sql import DataFrame
from pyspark.context import SparkContext
from functools import reduce, partial

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

"""
Glue Generic Job — Config local + PERFIS (profiles)

Como usar (exemplos):
  --CONFIG_PATH=configs/opportunity.yaml --PROFILE=prd --JOB_NAME=gc-sf-raw2sor-opportunity-prd

O job lê o YAML local dentro do repositório (pasta configs/ por exemplo),
seleciona o profile (dev/hml/prd), mescla overrides e executa:
RAW -> padroniza/alias -> hash/dedup -> DQ -> SOR.
"""

# ========== Logging ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(filename)s:%(lineno)d | %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Robust boolean coercion from YAML (accepts true/false, "true"/"false", 1/0, yes/no, on/off)
def as_bool(v, default=False):
    if isinstance(v, bool):
        return v
    if v is None:
        return default
    if isinstance(v, (int, float)):
        return v != 0
    if isinstance(v, str):
        return v.strip().lower() in {"1", "true", "t", "yes", "y", "on"}
    return default

# ========== Helpers ==========



def get_spark_type(type_name: str):
    """Converte nome textual em tipo Spark (ex.: 'StringType' -> T.StringType())."""
    if not type_name:
        return None
    if not hasattr(T, type_name):
        raise ValueError(f"Tipo Spark inválido no schema: {type_name}")
    return getattr(T, type_name)()


def get_bucket_only(s3_uri: str) -> str:
    u = urlparse(s3_uri)
    if u.scheme != "s3":
        raise ValueError(f"Esperado s3:// no caminho: {s3_uri}")
    return u.netloc


def read_text_local(path: str) -> str:
    """Lê arquivo local; tenta resolver com SparkFiles, caminho relativo ao script e /tmp.
    Aceita caminhos relativos (ex.: 'configs/opportunity.yaml') ou absolutos.
    """
    # 1) SparkFiles (quando arquivo foi adicionado como Referenced file)
    try:
        from pyspark import SparkFiles
        sf_try = SparkFiles.get(os.path.basename(path))
        if sf_try and os.path.exists(sf_try):
            with open(sf_try, "r", encoding="utf-8") as f:
                return f.read()
    except Exception:
        pass

    # 2) Caminhos candidatos (relativo ao script, /tmp e cwd)
    candidates = []
    if os.path.isabs(path):
        candidates.append(path)
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        candidates.append(os.path.join(base_dir, path))
        candidates.append(os.path.join("/tmp", path))
        candidates.append(path)  # relativo ao cwd

    for p in candidates:
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                return f.read()

    raise FileNotFoundError(
        f"Config não encontrada localmente. Tentado: SparkFiles({os.path.basename(path)}), {candidates}"
    )


def load_config_any(config_path: str) -> dict:
    """Carrega YAML/JSON de arquivo local (ou S3, se for s3://)."""
    if config_path.startswith("s3://"):
        bkt = get_bucket_only(config_path)
        key = config_path.replace(f"s3://{bkt}/", "", 1)
        s3 = boto3.client("s3")
        text = s3.get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
    else:
        text = read_text_local(config_path)

    if config_path.endswith((".yml", ".yaml")):
        import yaml
        return yaml.safe_load(text)
    return json.loads(text)


def deep_merge(base: dict, override: dict) -> dict:
    for k, v in (override or {}).items():
        if isinstance(v, dict) and isinstance(base.get(k), dict):
            deep_merge(base[k], v)
        else:
            base[k] = v
    return base


def apply_profile(cfg: dict, profile: str) -> dict:
    """Aplica overrides do profile (dev/hml/prd) sobre o cfg base."""
    profiles = cfg.get("profiles")
    if not profiles:
        return cfg
    if not profile:
        profile = cfg.get("default_profile", "prd")
    env_cfg = profiles.get(profile)
    if env_cfg is None:
        raise ValueError(f"PROFILE '{profile}' não encontrado em cfg.profiles")
    cfg2 = dict(cfg)
    cfg2.pop("profiles", None)
    cfg2.pop("default_profile", None)
    # mescla topo: paths, dq, write, window, read, schema, time, transform, etc.
    return deep_merge(cfg2, env_cfg)


# ========== Classe do Job ==========
class GlueGenericJob:
    def __init__(self, cfg: dict, job_args: dict):
        self.cfg = cfg
        self.job_args = job_args

        # ---- Config principal ----
        self.database = cfg["job"]["database"]
        self.table = cfg["job"]["table"]
        self.job_name = job_args.get("JOB_NAME", cfg["job"].get("name", "GlueGenericJob"))

        paths = cfg["paths"]
        self.bucket_raw = paths["bucket_raw"]  # s3://...
        self.folder_raw = paths["folder_raw"]  # prefixo
        self.bucket_sor = paths["bucket_sor"]  # s3://...
        self.folder_sor = paths["folder_sor"]  # prefixo

        window = cfg.get("window", {})
        self.force = as_bool(window.get("force", False))))
        self.start_date = window.get("start_date")  # YYYY-MM-DD
        self.end_date = window.get("end_date")
        self.days_retro = int(window.get("days_retro", 1))
        self.max_days_back = int(window.get("max_days_back", 720))

        read = cfg.get("read", {})
        self.input_format = read.get("input_format", "csv")
        csv = read.get("csv", {})
        self.csv_delim = csv.get("delimiter", ";")
        self.csv_header = str(csv.get("header", True)).lower()
        self.csv_encoding = csv.get("encoding", "utf-8")
        self.csv_multiline = str(csv.get("multiline", True)).lower()
        self.csv_quote = csv.get("quote_mode", "STOP_AT_CLOSING_QUOTE")
        self.csv_ignore_leading_ws  = str(csv.get("ignore_leading_whitespace", True)).lower()
        self.csv_ignore_trailing_ws = str(csv.get("ignore_trailing_whitespace", True)).lower()

        schema_cfg = cfg.get("schema", {})
        self.schema_cols = schema_cfg.get("columns") or {}
        if schema_cfg.get("path"):
            # também aceita arquivo local para schema
            path = schema_cfg["path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.schema_cols = json.loads(text) if path.endswith(".json") else __import__("yaml").safe_load(text)
        self.optional_cols = set(schema_cfg.get("optional_columns") or [])

        time_cfg = cfg.get("time", {})
        self.part_src_col = time_cfg.get("partition_source_col", "SystemModstamp")
        self.part_src_fmt = time_cfg.get("partition_source_fmt", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'")
        self.part_tgt_col = time_cfg.get("partition_target_col", "cod_patc")
        self.part_tgt_fmt = time_cfg.get("partition_target_format", "yyyyMMdd")
        self.tz_target = time_cfg.get("timezone_target")  # ex.: America/Sao_Paulo

        tr = cfg.get("transform", {})
        self.alias_map = tr.get("alias_map") or {}
        if tr.get("alias_map_path"):
            path = tr["alias_map_path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.alias_map = json.loads(text)
        self.hash_ignore = set(tr.get("hash_ignore_cols", ["des_arq", "dat_hor_psst", "cod_patc"]))

        write_cfg = cfg.get("write", {})
        self.output_format = write_cfg.get("output_format", "parquet")
        self.overwrite_mode = write_cfg.get("overwrite_mode", "dynamic")
        self.show_counts = as_bool(write_cfg.get("show_counts", False)))

        dq = cfg.get("dq", {})
        self.enable_dq = as_bool(dq.get("enable", True)))
        self.dq_rules_path = dq.get("rules_path")  # pode ser local ou s3

        # ---- Spark/Glue ----
        sc = SparkContext.getOrCreate()
        self.glue_context = GlueContext(sc)
        self.spark = self.glue_context.spark_session
        self.spark.conf.set("spark.sql.sources.partitionOverwriteMode", self.overwrite_mode)

        self.glue_client = boto3.client("glue")
        self.job = Job(self.glue_context)
        self.job.init(self.job_name, job_args)

        self.processed_partitions: list[str] = []  # yyyymmdd

        logger.info(f"Config para {self.database}.{self.table} carregada com sucesso (profile aplicado).")

    # --------- Intervalo de datas ---------
    def get_date_range(self):
        today_br = (datetime.utcnow() - timedelta(hours=3)).date()
        if self.force:
            if not (self.start_date and self.end_date):
                raise ValueError("force=true requer start_date e end_date no config.")
            ini = datetime.strptime(self.start_date, "%Y-%m-%d").date()
            fim = datetime.strptime(self.end_date, "%Y-%m-%d").date()
            if ini > fim:
                raise ValueError("start_date não pode ser maior que end_date.")
            return ini, fim

        last = self.get_last_partition()
        if not last:
            ini = today_br - timedelta(days=self.max_days_back)
        else:
            ini = datetime.strptime(str(last), "%Y%m%d").date()
        return ini, today_br

    def get_last_partition(self):
        paginator = self.glue_client.get_paginator("get_partitions")
        pages = paginator.paginate(DatabaseName=self.database, TableName=self.table)
        vals = []
        for page in pages:
            for p in page.get("Partitions", []):
                if p.get("Values"):
                    v = p["Values"][0]
                    if isinstance(v, str) and v.isdigit() and len(v) == 8:
                        vals.append(v)
        return max(vals) if vals else ""

    # --------- Leitura RAW (um dia) ---------
    def read_day(self, day: datetime) -> DataFrame | None:
        ymd = day.strftime("%Y%m%d")
        raw_bkt = get_bucket_only(self.bucket_raw)
        prefix = f"{self.folder_raw}/{ymd}"
        s3 = boto3.client("s3")

        objects = []
        token = None
        while True:
            kw = dict(Bucket=raw_bkt, Prefix=prefix, MaxKeys=1000)
            if token:
                kw["ContinuationToken"] = token
            resp = s3.list_objects_v2(**kw)
            objects.extend([c["Key"] for c in resp.get("Contents", [])])
            token = resp.get("NextContinuationToken")
            if not token:
                break

        if not objects:
            logger.info(f"Pasta vazia: {prefix}")
            return None

        self.processed_partitions.append(ymd)
        dfs = []
        for key in objects:
            path = f"s3://{raw_bkt}/{key}"
            logger.info(f"Lendo: {path}")
            reader = self.spark.read
            if self.input_format == "csv":
                reader = (
                    reader.option("encoding", self.csv_encoding)
                          .option("delimiter", self.csv_delim)
                          .option("header", self.csv_header)
                          .option("multiline", self.csv_multiline)
                          .option("unescapedQuoteHandling", self.csv_quote)
                          .option("ignoreLeadingWhiteSpace", self.csv_ignore_leading_ws)
                          .option("ignoreTrailingWhiteSpace", self.csv_ignore_trailing_ws)
                )
                df = reader.csv(path)
            else:
                df = reader.format(self.input_format).load(path)

            df = self.apply_schema(df)
            df = df.withColumn("descricao_arquivo", F.lit(path))
            df = df.withColumn("data_hora_processamento", F.current_timestamp())
            dfs.append(df)

        if not dfs:
            return None
        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        return reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]

    def apply_schema(self, df: DataFrame) -> DataFrame:
        if not self.schema_cols:
            return df
        for col_name, type_name in self.schema_cols.items():
            spark_type = get_spark_type(type_name)
            if col_name in df.columns:
                df = df.withColumn(col_name, F.col(col_name).cast(spark_type))
            else:
                df = df.withColumn(col_name, F.lit(None).cast(spark_type))
        return df

    # --------- Transformação / Partição / Alias ---------
    def transform(self, df: DataFrame) -> DataFrame:
        df = df.withColumn(
            "codigo_particao",
            F.date_format(F.to_timestamp(F.col(self.part_src_col), self.part_src_fmt), self.part_tgt_fmt),
        )

        # Ajuste de timezone (opcional) em campos comuns
        time_cols = [
            "CreatedDate",
            "LastModifiedDate",
            "SystemModstamp",
            "LastActivityDate",
            "LastStageChangeDate",
            "LastViewedDate",
            "LastReferencedDate",
        ]
        if self.tz_target:
            for c in time_cols:
                if c in df.columns:
                    df = df.withColumn(
                        c + "_adj",
                        F.from_utc_timestamp(
                            F.to_timestamp(F.col(c), self.part_src_fmt), self.tz_target
                        ),
                    )

        selects = []
        for src, dst in self.alias_map.items():
            if src in df.columns:
                selects.append(F.col(src).alias(dst))
            elif src.endswith("_adj") and src in df.columns:
                selects.append(F.col(src).alias(dst))
            else:
                if src in self.schema_cols:
                    selects.append(
                        F.lit(None).cast(get_spark_type(self.schema_cols[src])).alias(dst)
                    )

        selects.append(F.col("codigo_particao").cast(T.IntegerType()).alias(self.part_tgt_col))
        if "descricao_arquivo" in df.columns:
            selects.append(F.col("descricao_arquivo").cast(T.StringType()).alias("des_arq"))
        if "data_hora_processamento" in df.columns:
            selects.append(
                F.col("data_hora_processamento").cast(T.TimestampType()).alias("dat_hor_psst")
            )

        return df.select(*selects)

    # --------- Hash / Dedup ---------
    def add_hash(self, df: DataFrame) -> DataFrame:
        cols = sorted([c for c in df.columns if c not in self.hash_ignore])
        pieces = [F.coalesce(F.col(c).cast("string"), F.lit("∅")) for c in cols]
        return df.withColumn("des_ctrl_alter", F.sha2(F.concat_ws("", *pieces), 256))

    def remove_duplicates(self, df: DataFrame) -> DataFrame:
        return df.dropDuplicates(["des_ctrl_alter"])

    # --------- Escrita + DQ ---------
    def write(self, df: DataFrame):
        if df is None or df.rdd.isEmpty():
            logger.warning("Nada para escrever.")
            return
        dest_bkt = get_bucket_only(self.bucket_sor)
        dest = f"s3://{dest_bkt}/{self.folder_sor}/{self.table}"
        logger.info(f"Escrevendo em: {dest}")
        (
            df.write.mode("overwrite")
            .partitionBy(self.part_tgt_col)
            .format(self.output_format)
            .save(dest)
        )

        if self.enable_dq:
            self.write_dq_rules()

    def write_dq_rules(self):
        # Se um JSON de regras foi fornecido, copiar para o destino padrão
        if self.dq_rules_path:
            s3 = boto3.resource("s3")
            if self.dq_rules_path.startswith("s3://"):
                src_bkt = get_bucket_only(self.dq_rules_path)
                src_key = self.dq_rules_path.replace(f"s3://{src_bkt}/", "", 1)
                dst_bkt = get_bucket_only(self.bucket_sor)
                dst_key = f"motor_data_quality/{self.database}/{self.table}/{self.table}.json"
                s3.Object(dst_bkt, dst_key).copy_from(CopySource={"Bucket": src_bkt, "Key": src_key})
            else:
                # Local -> subir para S3
                text = read_text_local(self.dq_rules_path)
                dst_bkt = get_bucket_only(self.bucket_sor)
                dst_key = f"motor_data_quality/{self.database}/{self.table}/{self.table}.json"
                boto3.client("s3").put_object(Bucket=dst_bkt, Key=dst_key, Body=text.encode("utf-8"))
            logger.info("Regras DQ copiadas para o S3.")
            return

        # Regras básicas (fallback)
        rules = {
            "database": self.database,
            "table": self.table,
            "format": self.output_format,
            "partitions": [f"{self.part_tgt_col}={p}" for p in self.processed_partitions],
            "rules": [
                {"type": "RowCount", "operator": ">=", "expression": 1},
                {"type": "IsComplete", "column": "des_ctrl_alter"},
            ],
        }
        dst_bkt = get_bucket_only(self.bucket_sor)
        dst_key = f"motor_data_quality/{self.database}/{self.table}/{self.table}.json"
        boto3.client("s3").put_object(
            Bucket=dst_bkt, Key=dst_key, Body=json.dumps(rules).encode("utf-8")
        )
        logger.info(f"Regras DQ básicas escritas em s3://{dst_bkt}/{dst_key}")

    # --------- Orquestração ---------
    def run(self):
        ini, fim = self.get_date_range()
        logger.info(f"Janela de processamento: {ini} a {fim}")

        dfs = []
        d = ini
        while d <= fim:
            logger.info(f"Processando dia: {d}")
            df_day = self.read_day(datetime(d.year, d.month, d.day))
            if df_day is not None:
                dfs.append(df_day)
            d += timedelta(days=1)

        if not dfs:
            logger.warning("Sem dados no intervalo. Encerrando.")
            return

        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        df = reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]
        if self.show_counts:
            logger.info(f"Linhas brutas: {df.count()}")

        df = self.transform(df)
        df = self.add_hash(df)
        df = self.remove_duplicates(df)

        if self.show_counts:
            logger.info(f"Linhas pós-distinct: {df.count()}")

        self.write(df)

    def commit(self):
        self.job.commit()


# ========== main ==========
def main():
    # PROFILE é opcional; se não vier, usamos default_profile do YAML
    # getResolvedOptions exige chaves presentes; então tratamos PROFILE manualmente
    cli_profile = None
    for a in sys.argv:
        if a.startswith("--PROFILE="):
            cli_profile = a.split("=", 1)[1]
            break

    args = getResolvedOptions(sys.argv, ["CONFIG_PATH", "JOB_NAME"])  # obrigatórios
    if cli_profile:
        args["PROFILE"] = cli_profile

    cfg_raw = load_config_any(args["CONFIG_PATH"])  # YAML/JSON local (ou S3 se preferir)
    profile = args.get("PROFILE") or cfg_raw.get("default_profile", "prd")
    cfg = apply_profile(cfg_raw, profile)

    job = GlueGenericJob(cfg, args)
    try:
        job.run()
        job.commit()
        logger.info("Job concluído com sucesso.")
    except Exception:
        logger.exception("Falha na execução do job.")
        raise
    finally:
        logger.info("Fim do processamento.")


if __name__ == "__main__":
    main()
