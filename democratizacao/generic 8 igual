import sys, os, json, uuid, logging, boto3
from datetime import datetime, timedelta
from urllib.parse import urlparse

import pyspark.sql.functions as F
import pyspark.sql.types as T
from pyspark.sql import DataFrame
from pyspark.context import SparkContext
from functools import reduce, partial

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

"""
Glue Generic Job — Config LOCAL + Profiles (dev/hml/prd)

• Lê o YAML local ao repositório (ex.: configs/opportunity.yaml)
• Aplica o profile (--PROFILE=dev|hml|prd)
• RAW (S3) -> schema/cast -> alias (preferindo *_adj quando timezone) -> partição -> hash/dedup -> DQ (copy|basic|builder) -> SOR (S3)

Booleans (force, show_counts, dq.enable) são lidos DIRETO do YAML (sem defaults e sem coerção):
  window.force, write.show_counts, dq.enable
Se ausentes, o Python levantará KeyError naturalmente.

Extras desta versão (v2):
- Usa automaticamente colunas *_adj (ex.: CreatedDate_adj) quando time.timezone_target está definido.
- Suporta write.repartition (inteiro) para forçar N arquivos por partição.
- Suporta dq.mode: basic | copy | builder (o último integra RuleSetBuilder como no seu script).
- dat_hor_psst com timezone técnico (time.technical_timestamp_tz), se configurado.
"""

# ========== Logging ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(filename)s:%(lineno)d | %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ========== Helpers ==========

def get_spark_type(type_name: str):
    """Converte nome textual em tipo Spark (ex.: 'StringType' -> T.StringType())."""
    if not type_name:
        return None
    if not hasattr(T, type_name):
        raise ValueError(f"Tipo Spark inválido no schema: {type_name}")
    return getattr(T, type_name)()


def get_bucket_only(s3_uri: str) -> str:
    u = urlparse(s3_uri)
    if u.scheme != "s3":
        raise ValueError(f"Esperado s3:// no caminho: {s3_uri}")
    return u.netloc


def read_text_local(path: str) -> str:
    """Lê arquivo local; tenta SparkFiles, caminho relativo ao script, /tmp e cwd."""
    # 1) SparkFiles (quando arquivo foi adicionado como Referenced file)
    try:
        from pyspark import SparkFiles
        sf_try = SparkFiles.get(os.path.basename(path))
        if sf_try and os.path.exists(sf_try):
            with open(sf_try, "r", encoding="utf-8") as f:
                return f.read()
    except Exception:
        pass

    # 2) Caminhos candidatos (relativo ao script, /tmp e cwd)
    candidates = []
    if os.path.isabs(path):
        candidates.append(path)
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        candidates.append(os.path.join(base_dir, path))
        candidates.append(os.path.join("/tmp", path))
        candidates.append(path)  # relativo ao cwd

    for p in candidates:
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                return f.read()

    raise FileNotFoundError(
        f"Config não encontrada localmente. Tentado: SparkFiles({os.path.basename(path)}), {candidates}"
    )


def load_config_any(config_path: str) -> dict:
    """Carrega YAML/JSON de arquivo local (ou S3, se for s3://)."""
    if config_path.startswith("s3://"):
        bkt = get_bucket_only(config_path)
        key = config_path.replace(f"s3://{bkt}/", "", 1)
        s3 = boto3.client("s3")
        text = s3.get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
    else:
        text = read_text_local(config_path)

    if config_path.endswith((".yml", ".yaml")):
        import yaml
        return yaml.safe_load(text)
    return json.loads(text)


def deep_merge(base: dict, override: dict) -> dict:
    for k, v in (override or {}).items():
        if isinstance(v, dict) and isinstance(base.get(k), dict):
            deep_merge(base[k], v)
        else:
            base[k] = v
    return base


def apply_profile(cfg: dict, profile: str) -> dict:
    """Aplica overrides do profile (dev/hml/prd) sobre o cfg base."""
    profiles = cfg.get("profiles")
    if not profiles:
        return cfg
    if not profile:
        profile = cfg.get("default_profile", "prd")
    env_cfg = profiles.get(profile)
    if env_cfg is None:
        raise ValueError(f"PROFILE '{profile}' não encontrado em cfg.profiles")
    cfg2 = dict(cfg)
    cfg2.pop("profiles", None)
    cfg2.pop("default_profile", None)
    return deep_merge(cfg2, env_cfg)


# ========== Classe do Job ==========
class GlueGenericJob:
    def __init__(self, cfg: dict, job_args: dict):
        self.cfg = cfg
        self.job_args = job_args

        # ---- Config principal ----
        self.database = cfg["job"]["database"]
        self.table = cfg["job"]["table"]
        self.job_name = job_args.get("JOB_NAME", cfg["job"].get("name", "GlueGenericJob"))

        # paths são obrigatórios no YAML
        paths = cfg["paths"]
        self.bucket_raw = paths["bucket_raw"]  # s3://...
        self.folder_raw = paths["folder_raw"]  # prefixo
        self.bucket_sor = paths["bucket_sor"]  # s3://...
        self.folder_sor = paths["folder_sor"]  # prefixo

        # janela — force é obrigatório (bool real)
        window = cfg["window"]
        self.force = window["force"]  # bool direto do YAML
        self.start_date = window.get("start_date")  # YYYY-MM-DD (opcional)
        self.end_date = window.get("end_date")
        self.days_retro = int(window.get("days_retro", 1))
        self.max_days_back = int(window.get("max_days_back", 720))

        # leitura RAW
        read = cfg.get("read", {})
        self.input_format = read.get("input_format", "csv")
        csv = read.get("csv", {})
        self.csv_delim = csv.get("delimiter", ";")
        self.csv_header = str(csv.get("header", True)).lower()
        self.csv_encoding = csv.get("encoding", "utf-8")
        self.csv_multiline = str(csv.get("multiline", True)).lower()
        self.csv_quote = csv.get("quote_mode", "STOP_AT_CLOSING_QUOTE")
        self.csv_ignore_leading_ws  = str(csv.get("ignore_leading_whitespace", True)).lower()
        self.csv_ignore_trailing_ws = str(csv.get("ignore_trailing_whitespace", True)).lower()

        # schema
        schema_cfg = cfg.get("schema", {})
        self.schema_cols = schema_cfg.get("columns") or {}
        if schema_cfg.get("path"):
            path = schema_cfg["path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.schema_cols = json.loads(text) if path.endswith(".json") else __import__("yaml").safe_load(text)
        self.optional_cols = set(schema_cfg.get("optional_columns") or [])

        # tempo / partição
        time_cfg = cfg.get("time", {})
        self.part_src_col = time_cfg.get("partition_source_col", "SystemModstamp")
        self.part_src_fmt = time_cfg.get("partition_source_fmt", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'")
        self.part_tgt_col = time_cfg.get("partition_target_col", "cod_patc")
        self.part_tgt_fmt = time_cfg.get("partition_target_format", "yyyyMMdd")
        self.tz_target = time_cfg.get("timezone_target")
        self.tech_ts_tz = time_cfg.get("technical_timestamp_tz")

        # transform
        tr = cfg.get("transform", {})
        self.alias_map = tr.get("alias_map") or {}
        if tr.get("alias_map_path"):
            path = tr["alias_map_path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.alias_map = json.loads(text)
        self.hash_ignore = set(tr.get("hash_ignore_cols", ["des_arq", "dat_hor_psst", "cod_patc"]))

        # escrita — show_counts é obrigatório (bool real)
        write_cfg = cfg["write"]
        self.output_format = write_cfg.get("output_format", "parquet")
        self.overwrite_mode = write_cfg.get("overwrite_mode", "dynamic")
        self.show_counts = write_cfg["show_counts"]  # bool direto do YAML
        self.write_repartition = int(write_cfg.get("repartition", 0))

        # dq — enable é obrigatório (bool real)
        dq = cfg["dq"]
        self.enable_dq = dq["enable"]  # bool direto do YAML
        self.dq_mode = dq.get("mode", "basic")  # basic|copy|builder
        self.dq_rules_path = dq.get("rules_path")  # opcional (local ou s3)
        self.dq_builder = dq.get("builder", {})    # quando mode=builder

        # ---- Spark/Glue ----
        sc = SparkContext.getOrCreate()
        self.glue_context = GlueContext(sc)
        self.spark = self.glue_context.spark_session
        self.spark.conf.set("spark.sql.sources.partitionOverwriteMode", self.overwrite_mode)

        self.glue_client = boto3.client("glue")
        self.job = Job(self.glue_context)
        self.job.init(self.job_name, job_args)

        self.processed_partitions = []  # lista de yyyymmdd processados

        logger.info(
            f"Config para {self.database}.{self.table} carregada (profile aplicado)."
        )

    # --------- Intervalo de datas ---------
    def get_date_range(self):
        today_br = (datetime.utcnow() - timedelta(hours=3)).date()
        if self.force:
            if not (self.start_date and self.end_date):
                raise ValueError("force=true requer start_date e end_date no config.")
            ini = datetime.strptime(self.start_date, "%Y-%m-%d").date()
            fim = datetime.strptime(self.end_date, "%Y-%m-%d").date()
            if ini > fim:
                raise ValueError("start_date não pode ser maior que end_date.")
            return ini, fim

        last = self.get_last_partition()
        if not last:
            ini = today_br - timedelta(days=self.max_days_back)
        else:
            ini = datetime.strptime(str(last), "%Y%m%d").date()
        return ini, today_br

    def get_last_partition(self):
        paginator = self.glue_client.get_paginator("get_partitions")
        pages = paginator.paginate(DatabaseName=self.database, TableName=self.table)
        vals = []
        for page in pages:
            for p in page.get("Partitions", []):
                if p.get("Values"):
                    v = p["Values"][0]
                    if isinstance(v, str) and v.isdigit() and len(v) == 8:
                        vals.append(v)
        return max(vals) if vals else ""

    # --------- Leitura RAW (um dia) ---------
    def read_day(self, day: datetime) -> DataFrame | None:
        ymd = day.strftime("%Y%m%d")
        raw_bkt = get_bucket_only(self.bucket_raw)
        prefix = f"{self.folder_raw}/{ymd}"
        s3 = boto3.client("s3")

        objects = []
        token = None
        while True:
            kw = dict(Bucket=raw_bkt, Prefix=prefix, MaxKeys=1000)
            if token:
                kw["ContinuationToken"] = token
            resp = s3.list_objects_v2(**kw)
            objects.extend([c["Key"] for c in resp.get("Contents", [])])
            token = resp.get("NextContinuationToken")
            if not token:
                break

        if not objects:
            logger.info(f"Pasta vazia: {prefix}")
            return None

        self.processed_partitions.append(ymd)
        dfs = []
        for key in objects:
            path = f"s3://{raw_bkt}/{key}"
            logger.info(f"Lendo: {path}")
            reader = self.spark.read
            if self.input_format == "csv":
                reader = (
                    reader.option("encoding", self.csv_encoding)
                          .option("delimiter", self.csv_delim)
                          .option("header", self.csv_header)
                          .option("multiline", self.csv_multiline)
                          .option("unescapedQuoteHandling", self.csv_quote)
                          .option("ignoreLeadingWhiteSpace", self.csv_ignore_leading_ws)
                          .option("ignoreTrailingWhiteSpace", self.csv_ignore_trailing_ws)
                )
                df = reader.csv(path)
            else:
                df = reader.format(self.input_format).load(path)

            df = self.apply_schema(df)
            df = df.withColumn("descricao_arquivo", F.lit(path))
            # timestamp técnico: UTC ou timezone configurado
            if self.tech_ts_tz:
                df = df.withColumn(
                    "data_hora_processamento",
                    F.from_utc_timestamp(F.current_timestamp(), self.tech_ts_tz),
                )
            else:
                df = df.withColumn("data_hora_processamento", F.current_timestamp())
            dfs.append(df)

        if not dfs:
            return None
        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        return reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]

    def apply_schema(self, df: DataFrame) -> DataFrame:
        if not self.schema_cols:
            return df
        for col_name, type_name in self.schema_cols.items():
            spark_type = get_spark_type(type_name)
            if col_name in df.columns:
                df = df.withColumn(col_name, F.col(col_name).cast(spark_type))
            else:
                df = df.withColumn(col_name, F.lit(None).cast(spark_type))
        return df

    # --------- Transformação / Partição / Alias ---------
    def transform(self, df: DataFrame) -> DataFrame:
        # coluna de partição lógica
        df = df.withColumn(
            "codigo_particao",
            F.date_format(F.to_timestamp(F.col(self.part_src_col), self.part_src_fmt), self.part_tgt_fmt),
        )

        # Ajuste de timezone (opcional) – criamos *_adj
        if self.tz_target:
            time_cols = [
                "CreatedDate",
                "LastModifiedDate",
                "SystemModstamp",
                "LastActivityDate",
                "LastStageChangeDate",
                "LastViewedDate",
                "LastReferencedDate",
            ]
            for c in time_cols:
                if c in df.columns:
                    df = df.withColumn(
                        c + "_adj",
                        F.from_utc_timestamp(
                            F.to_timestamp(F.col(c), self.part_src_fmt), self.tz_target
                        ),
                    )

        def resolve_src(col_name: str):
            """Se timezone_target estiver setado e existir <col>_adj, usa _adj; senão usa o original."""
            if self.tz_target and (col_name + "_adj") in df.columns:
                return F.col(col_name + "_adj")
            return F.col(col_name)

        selects = []
        # aliases vindos do YAML
        for src, dst in self.alias_map.items():
            if src in df.columns or (self.tz_target and (src + "_adj") in df.columns):
                selects.append(resolve_src(src).alias(dst))
            else:
                # cria coluna nula tipada, se estiver no schema
                if src in self.schema_cols:
                    selects.append(
                        F.lit(None).cast(get_spark_type(self.schema_cols[src])).alias(dst)
                    )
        # coluna de partição física
        selects.append(F.col("codigo_particao").cast(T.IntegerType()).alias(self.part_tgt_col))
        # técnico: arquivo e horário de processamento
        if "descricao_arquivo" in df.columns:
            selects.append(F.col("descricao_arquivo").cast(T.StringType()).alias("des_arq"))
        if "data_hora_processamento" in df.columns:
            selects.append(
                F.col("data_hora_processamento").cast(T.TimestampType()).alias("dat_hor_psst")
            )

        return df.select(*selects)

    # --------- Hash / Dedup ---------
    def add_hash(self, df: DataFrame) -> DataFrame:
        cols = sorted([c for c in df.columns if c not in self.hash_ignore])
        pieces = [F.coalesce(F.col(c).cast("string"), F.lit("∅")) for c in cols]
        return df.withColumn("des_ctrl_alter", F.sha2(F.concat_ws("", *pieces), 256))

    def remove_duplicates(self, df: DataFrame) -> DataFrame:
        return df.dropDuplicates(["des_ctrl_alter"])

    # --------- Escrita + DQ ---------
    def write(self, df: DataFrame):
        if df is None or df.rdd.isEmpty():
            logger.warning("Nada para escrever.")
            return
        dest_bkt = get_bucket_only(self.bucket_sor)
        dest = f"s3://{dest_bkt}/{self.folder_sor}/{self.table}"
        logger.info(f"Escrevendo em: {dest}")

        df_to_write = df.repartition(self.write_repartition) if self.write_repartition > 0 else df
        (
            df_to_write.write.mode("overwrite")
            .partitionBy(self.part_tgt_col)
            .format(self.output_format)
            .save(dest)
        )

        if self.enable_dq:
            self.write_dq_rules()

    def write_dq_rules(self):
        dst_bkt = get_bucket_only(self.bucket_sor)
        dst_key = f"motor_data_quality/{self.database}/{self.table}/{self.table}.json"

        # 1) Modo builder – integra RuleSetBuilder (igual ao seu script)
        if self.dq_mode == "builder":
            try:
                from motordqconnector.dqconnector import RuleSetBuilder
            except Exception as e:
                logger.error("dq.mode=builder, mas não foi possível importar RuleSetBuilder: %s", e)
                raise

            partitions_list = [f"{self.part_tgt_col}={p}" for p in self.processed_partitions]
            s3_paths_list = [
                f"{self.bucket_sor}/{self.folder_sor}/{self.table}/{self.part_tgt_col}={p}"
                for p in self.processed_partitions
            ]
            owner = self.dq_builder.get("owner", "dataops")
            team = self.dq_builder.get("team", "dataops")

            builder = RuleSetBuilder(
                self.database,
                self.table,
                partitions_list,
                s3_paths_list,
                self.output_format,
                "True",
                str(uuid.uuid4()),
                team,
            )
            for rule in self.dq_builder.get("rules", []):
                rtype = rule.get("type")
                if not rtype:
                    continue
                if rtype == "IsComplete":
                    builder.add_rule("IsComplete", rule.get("column"))
                elif rtype == "RowCount":
                    builder.add_rule(
                        "RowCount",
                        operator=rule.get("operator", ">="),
                        expression=rule.get("expression", 1),
                    )
                elif rtype == "ColumnLength":
                    builder.add_rule(
                        "ColumnLength",
                        rule.get("column"),
                        operator=rule.get("operator", ">="),
                        expression=rule.get("expression", 1),
                    )
                else:
                    logger.warning("Tipo de regra DQ não suportado no builder: %s", rtype)

            ruleset_json = builder.build()
            boto3.client("s3").put_object(Bucket=dst_bkt, Key=dst_key, Body=ruleset_json.encode("utf-8"))
            logger.info("Regras DQ (builder) escritas em s3://%s/%s", dst_bkt, dst_key)
            return

        # 2) Modo copy – copia JSON pronto do caminho configurado
        if self.dq_mode == "copy" and self.dq_rules_path:
            s3 = boto3.resource("s3")
            if self.dq_rules_path.startswith("s3://"):
                src_bkt = get_bucket_only(self.dq_rules_path)
                src_key = self.dq_rules_path.replace(f"s3://{src_bkt}/", "", 1)
                s3.Object(dst_bkt, dst_key).copy_from(CopySource={"Bucket": src_bkt, "Key": src_key})
            else:
                text = read_text_local(self.dq_rules_path)
                boto3.client("s3").put_object(Bucket=dst_bkt, Key=dst_key, Body=text.encode("utf-8"))
            logger.info("Regras DQ copiadas para s3://%s/%s", dst_bkt, dst_key)
            return

        # 3) Modo basic – regras mínimas
        if self.dq_mode == "basic":
            rules = {
                "database": self.database,
                "table": self.table,
                "format": self.output_format,
                "partitions": [f"{self.part_tgt_col}={p}" for p in self.processed_partitions],
                "rules": [
                    {"type": "RowCount", "operator": ">=", "expression": 1},
                    {"type": "IsComplete", "column": "des_ctrl_alter"},
                ],
            }
            boto3.client("s3").put_object(
                Bucket=dst_bkt, Key=dst_key, Body=json.dumps(rules).encode("utf-8")
            )
            logger.info("Regras DQ básicas escritas em s3://%s/%s", dst_bkt, dst_key)

    # --------- Orquestração ---------
    def run(self):
        ini, fim = self.get_date_range()
        logger.info(f"Janela de processamento: {ini} a {fim}")

        dfs = []
        d = ini
        while d <= fim:
            logger.info(f"Processando dia: {d}")
            df_day = self.read_day(datetime(d.year, d.month, d.day))
            if df_day is not None:
                dfs.append(df_day)
            d += timedelta(days=1)

        if not dfs:
            logger.warning("Sem dados no intervalo. Encerrando.")
            return

        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        df = reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]
        if self.show_counts:
            logger.info(f"Linhas brutas: {df.count()}")

        df = self.transform(df)
        df = self.add_hash(df)
        df = self.remove_duplicates(df)

        if self.show_counts:
            logger.info(f"Linhas pós-distinct: {df.count()}")

        self.write(df)

    def commit(self):
        self.job.commit()


# ========== main ==========
def main():
    # PROFILE é opcional; se não vier, usa default_profile do YAML
    cli_profile = None
    for a in sys.argv:
        if a.startswith("--PROFILE="):
            cli_profile = a.split("=", 1)[1]
            break

    args = getResolvedOptions(sys.argv, ["CONFIG_PATH", "JOB_NAME"])  # obrigatórios
    if cli_profile:
        args["PROFILE"] = cli_profile

    cfg_raw = load_config_any(args["CONFIG_PATH"])  # YAML/JSON local (ou S3)
    profile = args.get("PROFILE") or cfg_raw.get("default_profile", "prd")
    cfg = apply_profile(cfg_raw, profile)

    job = GlueGenericJob(cfg, args)
    try:
        job.run()
        job.commit()
        logger.info("Job concluído com sucesso.")
    except Exception:
        logger.exception("Falha na execução do job.")
        raise
    finally:
        logger.info("Fim do processamento.")


if __name__ == "__main__":
    main()







--------------



def main():
    # PROFILE é opcional; capturamos manualmente (getResolvedOptions exige keys presentes)
    cli_profile = None
    for a in sys.argv:
        if a.startswith("--PROFILE="):
            cli_profile = a.split("=", 1)[1]
            break

    # Aqui pegamos DIRETO do Terraform (obrigatórios)
    args = getResolvedOptions(
        sys.argv,
        ["CONFIG_PATH", "JOB_NAME", "BUCKET_RAW", "FOLDER_RAW", "BUCKET_SOR", "FOLDER_SOR"]
    )
    if cli_profile:
        args["PROFILE"] = cli_profile

    # Carrega config e aplica profile (para o resto das chaves do YAML)
    cfg_raw = load_config_any(args["CONFIG_PATH"])
    profile = args.get("PROFILE") or cfg_raw.get("default_profile", "prd")
    cfg = apply_profile(cfg_raw, profile)

    job = GlueGenericJob(cfg, args)
    try:
        job.run()
        job.commit()
        logger.info("Job concluído com sucesso.")
    except Exception:
        logger.exception("Falha na execução do job.")
        raise
    finally:
        logger.info("Fim do processamento.")


if __name__ == "__main__":
    main()