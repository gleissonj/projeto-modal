# ===============================
# glue_generic_job.py (v3 — sem profiles)
# ===============================
import sys, os, json, uuid, logging, boto3
from datetime import datetime, timedelta
from urllib.parse import urlparse

import pyspark.sql.functions as F
import pyspark.sql.types as T
from pyspark.sql import DataFrame
from pyspark.context import SparkContext
from functools import reduce, partial

from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.utils import getResolvedOptions

"""
Glue Generic Job — Config LOCAL (sem profiles)

• Lê o YAML local ao repositório (ex.: configs/opportunity.yaml)
• RAW (S3) -> schema/cast -> alias (prioriza *_adj quando timezone) -> partição -> hash/dedup -> DQ (copy|basic|builder) -> SOR (S3)

Booleans (force, show_counts, dq.enable) são lidos DIRETO do YAML (sem defaults e sem coerção).
"""

# ========== Logging ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(filename)s:%(lineno)d | %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# ========== Helpers ==========

def get_spark_type(type_name: str):
    if not type_name:
        return None
    if not hasattr(T, type_name):
        raise ValueError(f"Tipo Spark inválido no schema: {type_name}")
    return getattr(T, type_name)()


def get_bucket_only(s3_uri: str) -> str:
    u = urlparse(s3_uri)
    if u.scheme != "s3":
        raise ValueError(f"Esperado s3:// no caminho: {s3_uri}")
    return u.netloc


def read_text_local(path: str) -> str:
    # 1) SparkFiles (quando arquivo foi adicionado como Referenced file)
    try:
        from pyspark import SparkFiles
        sf_try = SparkFiles.get(os.path.basename(path))
        if sf_try and os.path.exists(sf_try):
            with open(sf_try, "r", encoding="utf-8") as f:
                return f.read()
    except Exception:
        pass

    # 2) Caminhos candidatos (relativo ao script, /tmp e cwd)
    candidates = []
    if os.path.isabs(path):
        candidates.append(path)
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        candidates.append(os.path.join(base_dir, path))
        candidates.append(os.path.join("/tmp", path))
        candidates.append(path)

    for p in candidates:
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                return f.read()

    raise FileNotFoundError(
        f"Config não encontrada localmente. Tentado: SparkFiles({os.path.basename(path)}), {candidates}"
    )


def load_config_any(config_path: str) -> dict:
    if config_path.startswith("s3://"):
        bkt = get_bucket_only(config_path)
        key = config_path.replace(f"s3://{bkt}/", "", 1)
        s3 = boto3.client("s3")
        text = s3.get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
    else:
        text = read_text_local(config_path)

    if config_path.endswith((".yml", ".yaml")):
        import yaml
        return yaml.safe_load(text)
    return json.loads(text)

# ========== Classe do Job ==========
class GlueGenericJob:
    def __init__(self, cfg: dict, job_args: dict):
        self.cfg = cfg
        self.job_args = job_args

        # ---- Config principal ----
        self.database = cfg["job"]["database"]
        self.table = cfg["job"]["table"]
        self.job_name = job_args.get("JOB_NAME", cfg["job"].get("name", "GlueGenericJob"))

        # paths obrigatórios no YAML
        paths = cfg["paths"]
        self.bucket_raw = paths["bucket_raw"]  # s3://...
        self.folder_raw = paths["folder_raw"]  # prefixo
        self.bucket_sor = paths["bucket_sor"]  # s3://...
        self.folder_sor = paths["folder_sor"]  # prefixo

        # janela — booleans diretos do YAML
        window = cfg["window"]
        self.force = window["force"]
        self.start_date = window.get("start_date")
        self.end_date = window.get("end_date")
        self.days_retro = int(window.get("days_retro", 1))
        self.max_days_back = int(window.get("max_days_back", 720))

        # leitura RAW
        read = cfg.get("read", {})
        self.input_format = read.get("input_format", "csv")
        csv = read.get("csv", {})
        self.csv_delim = csv.get("delimiter", ";")
        self.csv_header = str(csv.get("header", True)).lower()
        self.csv_encoding = csv.get("encoding", "utf-8")
        self.csv_multiline = str(csv.get("multiline", True)).lower()
        self.csv_quote = csv.get("quote_mode", "STOP_AT_CLOSING_QUOTE")
        self.csv_ignore_leading_ws  = str(csv.get("ignore_leading_whitespace", True)).lower()
        self.csv_ignore_trailing_ws = str(csv.get("ignore_trailing_whitespace", True)).lower()

        # schema
        schema_cfg = cfg.get("schema", {})
        self.schema_cols = schema_cfg.get("columns") or {}
        if schema_cfg.get("path"):
            path = schema_cfg["path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.schema_cols = json.loads(text) if path.endswith(".json") else __import__("yaml").safe_load(text)
        self.optional_cols = set(schema_cfg.get("optional_columns") or [])

        # tempo / partição
        time_cfg = cfg.get("time", {})
        self.part_src_col = time_cfg.get("partition_source_col", "SystemModstamp")
        self.part_src_fmt = time_cfg.get("partition_source_fmt", "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'")
        self.part_tgt_col = time_cfg.get("partition_target_col", "cod_patc")
        self.part_tgt_fmt = time_cfg.get("partition_target_format", "yyyyMMdd")
        self.tz_target = time_cfg.get("timezone_target")
        self.tech_ts_tz = time_cfg.get("technical_timestamp_tz")

        # transform
        tr = cfg.get("transform", {})
        self.alias_map = tr.get("alias_map") or {}
        if tr.get("alias_map_path"):
            path = tr["alias_map_path"]
            if path.startswith("s3://"):
                bkt = get_bucket_only(path)
                key = path.replace(f"s3://{bkt}/", "", 1)
                text = boto3.client("s3").get_object(Bucket=bkt, Key=key)["Body"].read().decode("utf-8")
            else:
                text = read_text_local(path)
            self.alias_map = json.loads(text)
        self.hash_ignore = set(tr.get("hash_ignore_cols", ["des_arq", "dat_hor_psst", "cod_patc"]))

        # escrita
        write_cfg = cfg["write"]
        self.output_format = write_cfg.get("output_format", "parquet")
        self.overwrite_mode = write_cfg.get("overwrite_mode", "dynamic")
        self.show_counts = write_cfg["show_counts"]  # bool obrigatório no YAML
        self.write_repartition = int(write_cfg.get("repartition", 0))

        # dq
        dq = cfg["dq"]
        self.enable_dq = dq["enable"]  # bool obrigatório no YAML
        self.dq_mode = dq.get("mode", "basic")  # basic|copy|builder
        self.dq_rules_path = dq.get("rules_path")
        self.dq_builder = dq.get("builder", {})

        # ---- Spark/Glue ----
        sc = SparkContext.getOrCreate()
        self.glue_context = GlueContext(sc)
        self.spark = self.glue_context.spark_session
        self.spark.conf.set("spark.sql.sources.partitionOverwriteMode", self.overwrite_mode)

        self.glue_client = boto3.client("glue")
        self.job = Job(self.glue_context)
        self.job.init(self.job_name, job_args)

        self.processed_partitions = []
        logger.info(f"Config para {self.database}.{self.table} carregada.")

    # --------- Intervalo de datas ---------
    def get_date_range(self):
        today_br = (datetime.utcnow() - timedelta(hours=3)).date()
        if self.force:
            if not (self.start_date and self.end_date):
                raise ValueError("force=true requer start_date e end_date no config.")
            ini = datetime.strptime(self.start_date, "%Y-%m-%d").date()
            fim = datetime.strptime(self.end_date, "%Y-%m-%d").date()
            if ini > fim:
                raise ValueError("start_date não pode ser maior que end_date.")
            return ini, fim

        last = self.get_last_partition()
        if not last:
            ini = today_br - timedelta(days=self.max_days_back)
        else:
            ini = datetime.strptime(str(last), "%Y%m%d").date()
        return ini, today_br

    def get_last_partition(self):
        paginator = self.glue_client.get_paginator("get_partitions")
        pages = paginator.paginate(DatabaseName=self.database, TableName=self.table)
        vals = []
        for page in pages:
            for p in page.get("Partitions", []):
                if p.get("Values"):
                    v = p["Values"][0]
                    if isinstance(v, str) and v.isdigit() and len(v) == 8:
                        vals.append(v)
        return max(vals) if vals else ""

    # --------- Leitura RAW (um dia) ---------
    def read_day(self, day: datetime) -> DataFrame | None:
        ymd = day.strftime("%Y%m%d")
        raw_bkt = get_bucket_only(self.bucket_raw)
        prefix = f"{self.folder_raw}/{ymd}"
        s3 = boto3.client("s3")

        objects = []
        token = None
        while True:
            kw = dict(Bucket=raw_bkt, Prefix=prefix, MaxKeys=1000)
            if token:
                kw["ContinuationToken"] = token
            resp = s3.list_objects_v2(**kw)
            objects.extend([c["Key"] for c in resp.get("Contents", [])])
            token = resp.get("NextContinuationToken")
            if not token:
                break

        if not objects:
            logger.info(f"Pasta vazia: {prefix}")
            return None

        self.processed_partitions.append(ymd)
        dfs = []
        for key in objects:
            path = f"s3://{raw_bkt}/{key}"
            logger.info(f"Lendo: {path}")
            reader = self.spark.read
            if self.input_format == "csv":
                reader = (
                    reader.option("encoding", self.csv_encoding)
                          .option("delimiter", self.csv_delim)
                          .option("header", self.csv_header)
                          .option("multiline", self.csv_multiline)
                          .option("unescapedQuoteHandling", self.csv_quote)
                          .option("ignoreLeadingWhiteSpace", self.csv_ignore_leading_ws)
                          .option("ignoreTrailingWhiteSpace", self.csv_ignore_trailing_ws)
                )
                df = reader.csv(path)
            else:
                df = reader.format(self.input_format).load(path)

            df = self.apply_schema(df)
            df = df.withColumn("descricao_arquivo", F.lit(path))
            if self.tech_ts_tz:
                df = df.withColumn(
                    "data_hora_processamento",
                    F.from_utc_timestamp(F.current_timestamp(), self.tech_ts_tz),
                )
            else:
                df = df.withColumn("data_hora_processamento", F.current_timestamp())
            dfs.append(df)

        if not dfs:
            return None
        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        return reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]

    def apply_schema(self, df: DataFrame) -> DataFrame:
        if not self.schema_cols:
            return df
        for col_name, type_name in self.schema_cols.items():
            spark_type = get_spark_type(type_name)
            if col_name in df.columns:
                df = df.withColumn(col_name, F.col(col_name).cast(spark_type))
            else:
                df = df.withColumn(col_name, F.lit(None).cast(spark_type))
        return df

    # --------- Transformação / Partição / Alias ---------
    def transform(self, df: DataFrame) -> DataFrame:
        df = df.withColumn(
            "codigo_particao",
            F.date_format(F.to_timestamp(F.col(self.part_src_col), self.part_src_fmt), self.part_tgt_fmt),
        )

        if self.tz_target:
            time_cols = [
                "CreatedDate",
                "LastModifiedDate",
                "SystemModstamp",
                "LastActivityDate",
                "LastStageChangeDate",
                "LastViewedDate",
                "LastReferencedDate",
            ]
            for c in time_cols:
                if c in df.columns:
                    df = df.withColumn(
                        c + "_adj",
                        F.from_utc_timestamp(
                            F.to_timestamp(F.col(c), self.part_src_fmt), self.tz_target
                        ),
                    )

        def resolve_src(col_name: str):
            if self.tz_target and (col_name + "_adj") in df.columns:
                return F.col(col_name + "_adj")
            return F.col(col_name)

        selects = []
        for src, dst in self.alias_map.items():
            if src in df.columns or (self.tz_target and (src + "_adj") in df.columns):
                selects.append(resolve_src(src).alias(dst))
            else:
                if src in self.schema_cols:
                    selects.append(
                        F.lit(None).cast(get_spark_type(self.schema_cols[src])).alias(dst)
                    )
        selects.append(F.col("codigo_particao").cast(T.IntegerType()).alias(self.part_tgt_col))
        if "descricao_arquivo" in df.columns:
            selects.append(F.col("descricao_arquivo").cast(T.StringType()).alias("des_arq"))
        if "data_hora_processamento" in df.columns:
            selects.append(
                F.col("data_hora_processamento").cast(T.TimestampType()).alias("dat_hor_psst")
            )

        return df.select(*selects)

    # --------- Hash / Dedup ---------
    def add_hash(self, df: DataFrame) -> DataFrame:
        cols = sorted([c for c in df.columns if c not in self.hash_ignore])
        pieces = [F.coalesce(F.col(c).cast("string"), F.lit("∅")) for c in cols]
        return df.withColumn("des_ctrl_alter", F.sha2(F.concat_ws("", *pieces), 256))

    def remove_duplicates(self, df: DataFrame) -> DataFrame:
        return df.dropDuplicates(["des_ctrl_alter"])

    # --------- Escrita + DQ ---------
    def write(self, df: DataFrame):
        if df is None or df.rdd.isEmpty():
            logger.warning("Nada para escrever.")
            return
        dest_bkt = get_bucket_only(self.bucket_sor)
        dest = f"s3://{dest_bkt}/{self.folder_sor}/{self.table}"
        logger.info(f"Escrevendo em: {dest}")

        df_to_write = df.repartition(self.write_repartition) if self.write_repartition > 0 else df
        (
            df_to_write.write.mode("overwrite")
            .partitionBy(self.part_tgt_col)
            .format(self.output_format)
            .save(dest)
        )

        if self.enable_dq:
            self.write_dq_rules()

    def write_dq_rules(self):
        dst_bkt = get_bucket_only(self.bucket_sor)
        dst_key = f"motor_data_quality/{self.database}/{self.table}/{self.table}.json"

        if self.dq_mode == "builder":
            try:
                from motordqconnector.dqconnector import RuleSetBuilder
            except Exception as e:
                logger.error("dq.mode=builder, mas não foi possível importar RuleSetBuilder: %s", e)
                raise

            partitions_list = [f"{self.part_tgt_col}={p}" for p in self.processed_partitions]
            s3_paths_list = [
                f"{self.bucket_sor}/{self.folder_sor}/{self.table}/{self.part_tgt_col}={p}"
                for p in self.processed_partitions
            ]
            owner = self.dq_builder.get("owner", "dataops")
            team = self.dq_builder.get("team", "dataops")

            builder = RuleSetBuilder(
                self.database,
                self.table,
                partitions_list,
                s3_paths_list,
                self.output_format,
                "True",
                str(uuid.uuid4()),
                team,
            )
            for rule in self.dq_builder.get("rules", []):
                rtype = rule.get("type")
                if not rtype:
                    continue
                if rtype == "IsComplete":
                    builder.add_rule("IsComplete", rule.get("column"))
                elif rtype == "RowCount":
                    builder.add_rule(
                        "RowCount",
                        operator=rule.get("operator", ">="),
                        expression=rule.get("expression", 1),
                    )
                elif rtype == "ColumnLength":
                    builder.add_rule(
                        "ColumnLength",
                        rule.get("column"),
                        operator=rule.get("operator", ">="),
                        expression=rule.get("expression", 1),
                    )
                else:
                    logger.warning("Tipo de regra DQ não suportado no builder: %s", rtype)

            ruleset_json = builder.build()
            boto3.client("s3").put_object(Bucket=dst_bkt, Key=dst_key, Body=ruleset_json.encode("utf-8"))
            logger.info("Regras DQ (builder) escritas em s3://%s/%s", dst_bkt, dst_key)
            return

        if self.dq_mode == "copy" and self.dq_rules_path:
            s3 = boto3.resource("s3")
            if self.dq_rules_path.startswith("s3://"):
                src_bkt = get_bucket_only(self.dq_rules_path)
                src_key = self.dq_rules_path.replace(f"s3://{src_bkt}/", "", 1)
                s3.Object(dst_bkt, dst_key).copy_from(CopySource={"Bucket": src_bkt, "Key": src_key})
            else:
                text = read_text_local(self.dq_rules_path)
                boto3.client("s3").put_object(Bucket=dst_bkt, Key=dst_key, Body=text.encode("utf-8"))
            logger.info("Regras DQ copiadas para s3://%s/%s", dst_bkt, dst_key)
            return

        if self.dq_mode == "basic":
            rules = {
                "database": self.database,
                "table": self.table,
                "format": self.output_format,
                "partitions": [f"{self.part_tgt_col}={p}" for p in self.processed_partitions],
                "rules": [
                    {"type": "RowCount", "operator": ">=", "expression": 1},
                    {"type": "IsComplete", "column": "des_ctrl_alter"},
                ],
            }
            boto3.client("s3").put_object(
                Bucket=dst_bkt, Key=dst_key, Body=json.dumps(rules).encode("utf-8")
            )
            logger.info("Regras DQ básicas escritas em s3://%s/%s", dst_bkt, dst_key)

    # --------- Orquestração ---------
    def run(self):
        ini, fim = self.get_date_range()
        logger.info(f"Janela de processamento: {ini} a {fim}")

        dfs = []
        d = ini
        while d <= fim:
            logger.info(f"Processando dia: {d}")
            df_day = self.read_day(datetime(d.year, d.month, d.day))
            if df_day is not None:
                dfs.append(df_day)
            d += timedelta(days=1)

        if not dfs:
            logger.warning("Sem dados no intervalo. Encerrando.")
            return

        union_by_name = partial(type(dfs[0]).unionByName, allowMissingColumns=True)
        df = reduce(union_by_name, dfs) if len(dfs) > 1 else dfs[0]
        if self.show_counts:
            logger.info(f"Linhas brutas: {df.count()}")

        df = self.transform(df)
        df = self.add_hash(df)
        df = self.remove_duplicates(df)

        if self.show_counts:
            logger.info(f"Linhas pós-distinct: {df.count()}")

        self.write(df)

    def commit(self):
        self.job.commit()


# ========== main ==========
def main():
    args = getResolvedOptions(sys.argv, ["CONFIG_PATH", "JOB_NAME"])  # sem PROFILE
    cfg = load_config_any(args["CONFIG_PATH"])  # YAML/JSON local (ou S3)

    job = GlueGenericJob(cfg, args)
    try:
        job.run()
        job.commit()
        logger.info("Job concluído com sucesso.")
    except Exception:
        logger.exception("Falha na execução do job.")
        raise
    finally:
        logger.info("Fim do processamento.")


if __name__ == "__main__":
    main()


# ===============================
# opportunity.yaml (sem profiles)
# ===============================
# Salve como: configs/opportunity.yaml
---
version: 1

meta:
  owner_team_email: "eduardo.goncalves-silva@itau-unibanco.com.br"
  tech_team_email:  "43-engajamento.digital@Conectados.onmicrosoft.com"
  product_name: "democratizacao-salesforce-contatos"
  repository_name: "itau-kn8-app-gluejobsalesforce-contact"
  squad: "EngajamentoDigital"
  sigla: "kn8"
  servico_negocio: "democratizar dados do ibba 360"
  comunidade: "IBBA"

paths:
  bucket_raw: s3://itau-corp-artifact-sa-east-1-019163564458
  folder_raw: raw/salesforce/opportunity
  bucket_sor: s3://itau-corp-artifact-sa-east-1-019163564458
  folder_sor: sor/gestaocomercial/opportunity

job:
  name: gc-sf-raw2sor-oportunidade
  description: "Democratizar dados de oportunidade do IBBA 360"
  database: db_corp_gestaocomercial_governancasalesforceibba_sor_01
  table: tb_ibba360_evento_oportunidade

window:  # booleans obrigatórios
  force: false
  start_date: 2025-03-13
  end_date: 2025-09-04
  days_retro: 1
  max_days_back: 720

read:
  input_format: csv
  csv:
    delimiter: ";"
    header: true
    encoding: utf-8
    multiline: true
    quote_mode: STOP_AT_CLOSING_QUOTE
    ignore_leading_whitespace: true
    ignore_trailing_whitespace: true

schema:
  columns:
    AccountId: StringType
    Amount: DoubleType
    Budget_Confirmed__c: BooleanType
    CampaignId: StringType
    CloseDate: StringType
    CreatedById: StringType
    CreatedDate: StringType
    Description: StringType
    Discovery_Completed__c: BooleanType
    Fiscal: StringType
    FiscalQuarter: IntegerType
    FiscalYear: IntegerType
    ForecastCategory: StringType
    ForecastCategoryName: StringType
    HasOpenActivity: BooleanType
    HasOpportunityLineItem: BooleanType
    HasOverdueTask: BooleanType
    Id: StringType
    IsClosed: BooleanType
    IsDeleted: BooleanType
    IsWon: BooleanType
    LastActivityDate: StringType
    LastModifiedById: StringType
    LastModifiedDate: StringType
    LastStageChangeDate: StringType
    LastViewedDate: StringType
    LastReferencedDate: StringType
    LeadSource: StringType
    Name: StringType
    NextStep: StringType
    OwnerId: StringType
    Pricebook2Id: StringType
    Probability: IntegerType
    PushCount: IntegerType
    RecordTypeId: StringType
    StageName: StringType
    SyncedQuoteId: StringType
    SystemModstamp: StringType
    Territory2Id: StringType
    Type: StringType
    HU7_MotivoPerda__c: StringType
    HU7_SubMotivoPerda__c: StringType
    HU7_SpreadConcorrente__c: DoubleType
    HU7_Probabilidade__c: StringType
    AccountIdSubGrupo__c: StringType
    DataVencimentoConcorrente__c: StringType
    EspecialistaTime__c: StringType
    ExpectedRevenue: DoubleType
    HU7_Concorrente__c: StringType
    HU7_ContactReport__c: StringType
    HU7_Detalhes__c: StringType
    HU7_EmpresaGrupo__c: StringType
    HU7_EstimativaPotencial__c: DoubleType
    HU7_IDExterno__c: StringType
    HU7_IdVencimento__c: StringType
    HU7_NomeOferta__c: StringType
    HU7_Restrito__c: BooleanType
    InformacaoAdicional__c: StringType
    IsPrivate: BooleanType
    Participantes__c: StringType
    TotalOpportunityQuantity: StringType
  optional_columns: []

time:
  partition_source_col: SystemModstamp
  partition_source_fmt: "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"
  partition_target_col: cod_patc
  partition_target_format: yyyyMMdd
  timezone_target: America/Sao_Paulo           # ativa *_adj e preferência automática
  technical_timestamp_tz: America/Sao_Paulo    # dat_hor_psst em BR

transform:
  alias_map:
    CreatedDate: dat_cria
    LastActivityDate: dat_ulti_atii
    LastModifiedDate: dat_ulti_alte
    LastReferencedDate: dat_ulti_rfrc
    LastStageChangeDate: dat_ulti_muda_esta
    LastViewedDate: dat_ulti_visu
    SystemModstamp: dat_alte_dado
    AccountId: num_idt_cont
    Amount: vlr_esmd_vend
    Budget_Confirmed__c: ind_orcm_cfmd_c
    CampaignId: num_idt_camp
    CloseDate: dat_fecm
    CreatedById: des_idt_ctto
    Description: des_optd
    Discovery_Completed__c: ind_dsro_cocd_c
    Fiscal: des_ano_fisc
    FiscalQuarter: num_trme_fisc
    FiscalYear: ano_fisc
    ForecastCategory: cate_prvs
    ForecastCategoryName: nom_cate_prvs
    HasOpenActivity: ind_atii_aber
    HasOpportunityLineItem: ind_item_linh_asdo
    HasOverdueTask: ind_tare_vncd
    Id: cod_idt
    IsClosed: ind_optd_fech
    IsDeleted: idt_excu
    IsWon: ind_optd_cnqt
    LastModifiedById: cod_usua_ulti_alte
    LeadSource: nom_font_optd
    Name: des_nom
    NextStep: des_prox_etap
    OwnerId: num_idt_usua_optd
    Pricebook2Id: num_idt_price_book
    Probability: pbbbl_fecm_optd
    PushCount: num_cotg_envo
    RecordTypeId: cod_tipo_rgto
    StageName: des_idt_ctotc_snca
    SyncedQuoteId: cod_idt_trto
    Territory2Id: num_tipo
    Type: tip_oprt
    HU7_MotivoPerda__c: txt_moti_perd
    HU7_SubMotivoPerda__c: txt_sub_moti_perd
    HU7_SpreadConcorrente__c: num_sprd_crt
    HU7_Probabilidade__c: nom_pbbbl
    AccountIdSubGrupo__c: num_idt_cont_subg
    DataVencimentoConcorrente__c: dat_venc_conc_c
    EspecialistaTime__c: esp_time_c
    ExpectedRevenue: exp_reve
    HU7_Concorrente__c: cod_idt_conc_c
    HU7_ContactReport__c: cod_idt_ctc_repo_c
    HU7_Detalhes__c: txt_deta_c
    HU7_EmpresaGrupo__c: cod_idt_emp_grp_c
    HU7_EstimativaPotencial__c: num_est_pote_c
    HU7_IDExterno__c: cod_idt_exte_c
    HU7_IdVencimento__c: cod_idt_venc_c
    HU7_NomeOferta__c: nom_ofta_c
    HU7_Restrito__c: rest_c
    InformacaoAdicional__c: inf_adic_c
    IsPrivate: is_priv
    Participantes__c: txt_part_c
    TotalOpportunityQuantity: ttl_opp_qnty
  hash_ignore_cols: [ des_arq, dat_hor_psst, cod_patc ]

write:
  output_format: parquet
  overwrite_mode: dynamic
  show_counts: false   # obrigatório (sem default no código)
  repartition: 0       # 0 = sem repartition; use 1 para 1 arquivo/partição

dq:
  enable: true         # obrigatório (sem default no código)
  mode: builder        # builder|copy|basic
  builder:
    owner: dataops
    team: ibba360
    rules:
      - type: IsComplete
        column: cod_idt
      - type: IsComplete
        column: des_ctrl_alter
      - type: RowCount
        operator: ">="
        expression: 1
      - type: ColumnLength
        column: des_ctrl_alter
        operator: ">="
        expression: 1
